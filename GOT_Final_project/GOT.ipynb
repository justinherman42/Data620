{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " \n",
    " For this project we are going to use NLP and network grpahs to analyze the Game Of Thrones books.  All book files have been uploaded to github here: https://github.com/justinherman42/Data620/tree/master/GOT_Final_project/textfiles\n",
    " \n",
    " The text files were edited by hand to start with the Prologue, thus discarding some summary data from previous books.  Our plan is to use name entity recognition to visualize the relationships between characters throughout the book series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_file):    \n",
    "\"\"\"\n",
    "Reads in text file from GITHUB gets rid of newline and special characters\n",
    "\"\"\"\n",
    "    url = 'https://raw.githubusercontent.com/justinherman42/Data620/master/GOT_Final_project/textfiles/'+text_file\n",
    "    page = requests.get(url)\n",
    "    text2=page.text\n",
    "    data = text2.replace('\\r', ' ').replace('\\n', ' ').replace(\"\\'\", \"'\")   \n",
    "    novel=''\n",
    "    novel += ' ' + data\n",
    "    novel = novel.replace('. . .',';')\n",
    "    return (novel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store text in dictionary\n",
    "+ Built out dictionary with untokenized data in case you want to look at it\n",
    "+ All I really saw in terms of issues was '...' being frequently used in the books.  Obviously this messed with the tokenizer, so i replaced it with  ; in the clean text function above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## build out names for text file URL's\n",
    "git_text_file_names=['got1.txt','got2.txt','got3.txt','got4.txt','got5.txt']\n",
    "\n",
    "## Built out dictionary with untokenized data in case you want to look at it\n",
    "## All I really saw in terms of issues was ... being frequently used which I substituted with ;\n",
    "\n",
    "book_series_untokenized={}\n",
    "for i,x in enumerate(git_text_file_names):\n",
    "    book_series_untokenized[\"book{}\".format(i)]=clean_text(x)\n",
    "\n",
    "## built out dictionary with tokenized data\n",
    "book_series={}\n",
    "for i,x in enumerate(git_text_file_names):\n",
    "    orig_text=clean_text(x)\n",
    "    sentence_list = sent_tokenize(orig_text)\n",
    "    book_series[\"book{}\".format(i+1)]= sentence_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to work on a book, call the dictionary value. I.E 'Book1','Book2','Book3'\n",
    "type(book_series[\"book1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wasnt loading in github so commented out\n",
    "\n",
    "#book_series[\"book1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wasnt loading in github so commented out\n",
    "#book_series[\"book2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Function is taken directly form Harry Potter project\n",
    "+ I added the common words file directly in line as well, I could upload to git instead\n",
    "+ Let me know if you guys want access to the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def name_entity_recognition(sentence):\n",
    "    '''\n",
    "    A function to retrieve name entities in a sentence.\n",
    "    :param sentence: the sentence to retrieve names from.\n",
    "    :return: a name entity list of the sentence.\n",
    "    '''\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    # retrieve person and organization's name from the sentence\n",
    "    name_entity = [x for x in doc.ents if x.label_ in ['PERSON', 'ORG']]\n",
    "    # convert all names to lowercase and remove 's in names\n",
    "    name_entity = [str(x).lower().replace(\"'s\",\"\") for x in name_entity]\n",
    "    # split names into single words ('Harry Potter' -> ['Harry', 'Potter'])\n",
    "    name_entity = [x.split(' ') for x in name_entity]\n",
    "    # flatten the name list\n",
    "    name_entity = flatten(name_entity)\n",
    "    # remove name words that are less than 3 letters to raise recognition accuracy\n",
    "    name_entity = [x for x in name_entity if len(x) >= 3]\n",
    "    # remove name words that are in the set of 4000 common words\n",
    "    name_entity = [x for x in name_entity if x not in words]\n",
    "\n",
    "    return name_entity\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "672.646px",
    "left": "2402px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
